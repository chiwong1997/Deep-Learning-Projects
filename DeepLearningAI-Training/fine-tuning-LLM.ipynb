{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning LLM project \n",
    "\\\n",
    "\\\n",
    "Uses LLAMA from Meta\n",
    "\\\n",
    "A basic finetuning project, to try out finetuning, and to learn the basics of finetuning through Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cwong2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lamini\n",
    "from lamini import Lamini\n",
    "\n",
    "import datasets #this library allows easy access to datasets on HuggingFace Datasets Hub\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "lamini.api_key = \"ae666640b62327d2dff4c6e147cc34c6c0cecb0b8e7887ffe49a795104980587\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will look at the llama model from meta. We will look at both their regular llm, and llm that has been trained to be a chatbot. We will be looking at LLama-2-7b-hf (https://huggingface.co/meta-llama/Llama-2-7b-hf) and LLama-2-7b-chat-hf which are the fine tuned models that are optimized for chat use cases. Later we will look at also look at Meta-LLama-3.1-8B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_llm = Lamini(model_name=\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Who are the best NBA players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now?\n",
      "The NBA is a league full of superstars, but who are the best players in the league right now?\n",
      "The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now?\n",
      "The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of superstars, but who are the best players in the league right now? The NBA is a league full of\n"
     ]
    }
   ],
   "source": [
    "print(nf_llm.generate(\"Who are the best NBA players in the league right now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the fine tuned output now - these have just been fine-tuned by meta themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The NBA is home to some of the most talented athletes in the world, and there are several players who are currently dominating the league. Here are some of the best NBA players in the league right now:\n",
      "\n",
      "1. Luka Doncic - Dallas Mavericks: Doncic is a 22-year-old phenom who is leading the NBA in scoring, averaging 31.7 points, 9.7 rebounds, and 8.7 assists per game. He is a versatile player who can dominate games on both ends of the court.\n",
      "2. Kevin Durant - Brooklyn Nets: Durant is a 32-year-old forward who is known for his scoring ability and athleticism. He is averaging 26.9 points, 7.1 rebounds, and 5.9 assists per game, and is a four-time NBA scoring champion.\n",
      "3. Giannis Antetokounmpo - Milwaukee Bucks: Antetokounmpo is a 26-year-old forward who is one of the most dominant players in the league. He is averaging 29.6 points, 12.7 rebounds, and 5.9 assists per game, and is a two-time NBA Most Valuable Player.\n",
      "4. Stephen Curry - Golden State Warriors: Curry is a 33-year-old guard who is known for his shooting ability and intelligence on the court. He is averaging 27.3 points, 5.3 rebounds, and 5.2 assists per game, and is a three-time NBA champion.\n",
      "5. James Harden - Brooklyn Nets: Harden is a 32-year-old guard who is one of the most prolific scorers in the league. He is averaging 25.3 points, 7.5 rebounds, and 7.9 assists per game, and is a four-time NBA scoring champion.\n",
      "6. Nikola Jokic - Denver Nuggets: Jokic is a 25-year-old center who is one of the most versatile players in the league. He is averaging 20.1 points, 10.8 rebounds, and 7.7 assists per game, and is a two-time NBA All-Star.\n",
      "7. Joel Embiid - Philadelphia 76ers: Embiid is a 27-year-old center who is one of the most dominant players in the league. He is averaging 22.3 points, 10.6 rebounds, and 2.6 blocks per game, and is a two-time NBA All-Star.\n",
      "8. Damian Lillard - Portland Trail Blazers: Lillard is a 31-year-old guard who is known for his scoring ability and clutch performances. He is averaging 22.3 points, 4.6 rebounds, and 7.5 assists per game, and is a four-time NBA All-Star.\n",
      "9. Kyrie Irving - Brooklyn Nets: Irving is a 30-year-old guard who is known for his shooting ability and playmaking skills. He is averaging 20.4 points, 4.5 rebounds, and 6.4 assists per game, and is a four-time NBA All-Star.\n",
      "10. Paul George - Los Angeles Clippers: George is a 31-year-old forward who is known for his scoring ability and defensive prowess. He is averaging 20.4 points, 5.7 rebounds, and 4.3 assists per game, and is a four-time NBA All-Star.\n",
      "\n",
      "Of course, there are many other talented players in the NBA, and the rankings can vary depending on individual opinions and statistics. However, these 10 players are generally considered to be among the best in the league right now.\n"
     ]
    }
   ],
   "source": [
    "f_llm = Lamini(model_name=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "print(f_llm.generate(\"Who are the best NBA players in the league right now?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is a 7-day workout plan for you as a basketball player who wants to get in shape:\n",
      "\n",
      "Day 1: Cardio and Strength Training\n",
      "\n",
      "* Warm-up: 5-10 minutes of light cardio (jogging, jumping jacks, etc.)\n",
      "* Strength training: 3 sets of 10-15 reps of exercises like squats, lunges, push-ups, and rows\n",
      "* Cardio: 30 minutes of steady-state cardio (jogging, cycling, etc.)\n",
      "* Cool-down: 5-10 minutes of stretching\n",
      "\n",
      "Day 2: Rest day\n",
      "\n",
      "Day 3: Plyometric Training\n",
      "\n",
      "* Warm-up: 5-10 minutes of light cardio\n",
      "* Plyometric training: 3 sets of 10-15 reps of exercises like box jumps, depth jumps, and burpees\n",
      "* Cardio: 30 minutes of steady-state cardio\n",
      "* Cool-down: 5-10 minutes of stretching\n",
      "\n",
      "Day 4: Strength Training\n",
      "\n",
      "* Warm-up: 5-10 minutes of light cardio\n",
      "* Strength training: 3 sets of 10-15 reps of exercises like deadlifts, bench press, and rows\n",
      "* Cardio: 30 minutes of steady-state cardio\n",
      "* Cool-down: 5-10 minutes of stretching\n",
      "\n",
      "Day 5: Rest day\n",
      "\n",
      "Day 6: Agility Training\n",
      "\n",
      "* Warm-up: 5-10 minutes of light cardio\n",
      "* Agility training: 3 sets of 10-15 reps of exercises like cone drills, ladder drills, and shuttle runs\n",
      "* Cardio: 30 minutes of steady-state cardio\n",
      "* Cool-down: 5-10 minutes of stretching\n",
      "\n",
      "Day 7: Rest day\n",
      "\n",
      "This workout plan is designed to help you improve your cardiovascular fitness, strength, and agility while also allowing for adequate rest and recovery time. It's important to listen to your body and adjust the intensity and volume of your workouts based on how you feel. Remember to also eat a balanced diet and stay hydrated to support your training.\n",
      "\n",
      "As a basketball player, it's important to focus on exercises that mimic the movements and actions you use on the court. This workout plan includes a mix of strength training, plyometric training, and agility training to help you improve your overall fitness and performance.\n",
      "\n",
      "Remember to also include rest days in your training plan to allow your body to recover and rebuild. This will help you avoid overtraining and reduce your risk of injury.\n",
      "\n",
      "I hope this workout plan helps you achieve your fitness goals! Let me know if you have any questions or need further assistance.\n"
     ]
    }
   ],
   "source": [
    "print(f_llm.generate(\"I am a basketball player who wants to get in shape. Write a 7 day workout plan for me that would give me enough rest, while also improving my cardio, taking into account that I play two games a week on Tuesday and Wednesday.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see. Can you please provide me with your order number so I can look into this for you?\n",
      "Customer: I don't have the order number.\n",
      "Agent: Okay, no worries. Can you please tell me the name of the item you didn't receive?\n",
      "Customer: It was a blue blanket.\n",
      "Agent: I see. I'm going to check on the status of your order for you. Can you please hold for just a moment?\n",
      "Customer: Okay, thank you.\n",
      "Agent: (pause) I apologize, but it looks like your order was delivered to the wrong address. Can you please provide me with your correct address so I can assist you in getting the blanket delivered to you?\n",
      "Customer: (sighs) I don't know why this is happening to me. I've been ordering from Amazon for years and never had any problems.\n",
      "Agent: I apologize for any inconvenience this has caused. I'm here to help you resolve the issue as quickly as possible. Can you please provide me with your correct address so I can assist you?\n",
      "Customer: (frustrated) Fine. It's 123 Main St, Apartment 302. Can you please fix this and get my blanket delivered to me?\n",
      "Agent: (nodding) Of course, I understand. I'm going to go ahead and update the address on your order. Can you please hold for just a moment while I process this?\n",
      "Customer: (sighs) Okay, thank you.\n",
      "Agent: (pause) Great, I've updated the address on your order. Your blanket should be delivered to you within the next 3-5 business days. Is there anything else I can assist you with?\n",
      "Customer: (disappointed) No, that's all. Thank you for your help.\n",
      "Agent: You're welcome. Is there anything else I can do for you?\n",
      "Customer: (hesitates) No, that's all. Thank you again.\n",
      "Agent: You're welcome. Have a great day!\n",
      "\n",
      "In this scenario, the customer service agent is trying to help the customer resolve an issue with their Amazon delivery order. The customer did not receive the item they ordered, and the agent is working to update the address on the order to ensure that the customer receives their item as soon as possible. The agent is empathetic and professional throughout the conversation, and provides clear and concise explanations of the steps they are taking to resolve the issue.\n"
     ]
    }
   ],
   "source": [
    "print(f_llm.generate(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
    "Customer: I didn't get my item\n",
    "Agent: I'm sorry to hear that. Which item was it?\n",
    "Customer: the blanket\n",
    "Agent:\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pretrained dataset and compare it with a finetuning dataset (taken from HuggingFace) - we will use streaming=True as the dataset is huge, so we don't want to load the entire dataset in at once, so we stream one line in at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cwong2\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\c4\\584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db\\c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pretrained_dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Beginners BBQ Class Taking Place in Missoula!\\nDo you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\\nHe will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\\nThe cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.', 'timestamp': '2019-04-25T12:57:54Z', 'url': 'https://klyq.com/beginners-bbq-class-taking-place-in-missoula/'}\n",
      "{'text': 'Discussion in \\'Mac OS X Lion (10.7)\\' started by axboi87, Jan 20, 2012.\\nI\\'ve got a 500gb internal drive and a 240gb SSD.\\nWhen trying to restore using disk utility i\\'m given the error \"Not enough space on disk ____ to restore\"\\nBut I shouldn\\'t have to do that!!!\\nAny ideas or workarounds before resorting to the above?\\nUse Carbon Copy Cloner to copy one drive to the other. I\\'ve done this several times going from larger HDD to smaller SSD and I wound up with a bootable SSD drive. One step you have to remember not to skip is to use Disk Utility to partition the SSD as GUID partition scheme HFS+ before doing the clone. If it came Apple Partition Scheme, even if you let CCC do the clone, the resulting drive won\\'t be bootable. CCC usually works in \"file mode\" and it can easily copy a larger drive (that\\'s mostly empty) onto a smaller drive. If you tell CCC to clone a drive you did NOT boot from, it can work in block copy mode where the destination drive must be the same size or larger than the drive you are cloning from (if I recall).\\nI\\'ve actually done this somehow on Disk Utility several times (booting from a different drive (or even the dvd) so not running disk utility from the drive your cloning) and had it work just fine from larger to smaller bootable clone. Definitely format the drive cloning to first, as bootable Apple etc..\\nThanks for pointing this out. My only experience using DU to go larger to smaller was when I was trying to make a Lion install stick and I was unable to restore InstallESD.dmg to a 4 GB USB stick but of course the reason that wouldn\\'t fit is there was slightly more than 4 GB of data.', 'timestamp': '2019-04-21T10:07:13Z', 'url': 'https://forums.macrumors.com/threads/restore-from-larger-disk-to-smaller-disk.1311329/'}\n",
      "{'text': 'Foil plaid lycra and spandex shortall with metallic slinky insets. Attached metallic elastic belt with O-ring. Headband included. Great hip hop or jazz dance costume. Made in the USA.', 'timestamp': '2019-04-25T10:40:23Z', 'url': 'https://awishcometrue.com/Catalogs/Clearance/Tweens/V1960-Find-A-Way'}\n",
      "{'text': \"How many backlinks per day for new site?\\nDiscussion in 'Black Hat SEO' started by Omoplata, Dec 3, 2010.\\n1) for a newly created site, what's the max # backlinks per day I should do to be safe?\\n2) how long do I have to let my site age before I can start making more blinks?\\nI did about 6000 forum profiles every 24 hours for 10 days for one of my sites which had a brand new domain.\\nThere is three backlinks for every of these forum profile so thats 18 000 backlinks every 24 hours and nothing happened in terms of being penalized or sandboxed. This is now maybe 3 months ago and the site is ranking on first page for a lot of my targeted keywords.\\nbuild more you can in starting but do manual submission and not spammy type means manual + relevant to the post.. then after 1 month you can make a big blast..\\nWow, dude, you built 18k backlinks a day on a brand new site? How quickly did you rank up? What kind of competition/searches did those keywords have?\", 'timestamp': '2019-04-21T12:46:19Z', 'url': 'https://www.blackhatworld.com/seo/how-many-backlinks-per-day-for-new-site.258615/'}\n",
      "{'text': 'The Denver Board of Education opened the 2017-18 school year with an update on projects that include new construction, upgrades, heat mitigation and quality learning environments.\\nWe are excited that Denver students will be the beneficiaries of a four year, $572 million General Obligation Bond. Since the passage of the bond, our construction team has worked to schedule the projects over the four-year term of the bond.\\nDenver voters on Tuesday approved bond and mill funding measures for students in Denver Public Schools, agreeing to invest $572 million in bond funding to build and improve schools and $56.6 million in operating dollars to support proven initiatives, such as early literacy.\\nDenver voters say yes to bond and mill levy funding support for DPS students and schools. Click to learn more about the details of the voter-approved bond measure.\\nDenver voters on Nov. 8 approved bond and mill funding measures for DPS students and schools. Learn more about what’s included in the mill levy measure.', 'timestamp': '2019-04-20T14:33:21Z', 'url': 'http://bond.dpsk12.org/category/news/'}\n"
     ]
    }
   ],
   "source": [
    "n=5\n",
    "top_n = itertools.islice(pretrained_dataset, n)\n",
    "for i in top_n:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the different types of documents avai...</td>\n",
       "      <td>Lamini has documentation on Getting Started, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the recommended way to set up and conf...</td>\n",
       "      <td>Lamini can be downloaded as a python package a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find the specific documentation I ne...</td>\n",
       "      <td>You can ask this model about documentation, wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does the documentation include explanations of...</td>\n",
       "      <td>Our documentation provides both real-world and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does the documentation provide information abo...</td>\n",
       "      <td>External dependencies and libraries are all av...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What are the different types of documents avai...   \n",
       "1  What is the recommended way to set up and conf...   \n",
       "2  How can I find the specific documentation I ne...   \n",
       "3  Does the documentation include explanations of...   \n",
       "4  Does the documentation provide information abo...   \n",
       "\n",
       "                                              answer  \n",
       "0  Lamini has documentation on Getting Started, A...  \n",
       "1  Lamini can be downloaded as a python package a...  \n",
       "2  You can ask this model about documentation, wh...  \n",
       "3  Our documentation provides both real-world and...  \n",
       "4  External dependencies and libraries are all av...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_data_df = pd.read_json(\"lamini_docs.jsonl\", lines=True)\n",
    "instruction_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting the dataset into a dictionary \n",
    "examples = instruction_data_df.to_dict()\n",
    "# Taking a look at the first element in the dictionary, formatting into a line of text by concatenation\n",
    "text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other ways of formatting\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "    text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "    text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "    text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "    text = examples[\"text\"][0]\n",
    "\n",
    "# We want to give more structure, so that it helps the model \n",
    "# Writing a prompt template to hold both the questions and answers - the ### delimiters are used to help the model \n",
    "prompt_template_qa = \"\"\"### Question: \n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "{answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Question: \\nWhat are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\\n\\n### Answer:\\nLamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = examples[\"question\"][0]\n",
    "answer = examples[\"answer\"][0]\n",
    "\n",
    "text_with_prompt_template = prompt_template_qa.format(question = question, answer = answer)\n",
    "text_with_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a prompt template to hold just the question - helps to have this for evaluation\n",
    "prompt_template_q = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Question:\\nWhat are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\\n\\n### Answer:\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_q.format(question=question, answer=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the template to the entire dataset\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset_text_only = []\n",
    "finetuning_dataset_question_answer = []\n",
    "for i in range(num_examples):\n",
    "    question = examples[\"question\"][i]\n",
    "    answer = examples[\"answer\"][i]\n",
    "\n",
    "    text_with_prompt_template_qa = prompt_template_qa.format(question=question, answer=answer)\n",
    "    finetuning_dataset_text_only.append({\"text\": text_with_prompt_template_qa})\n",
    "\n",
    "    text_with_prompt_template_q = prompt_template_q.format(question=question)\n",
    "    finetuning_dataset_question_answer.append({\"question\": text_with_prompt_template_q, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '### Question: \\n'\n",
      "         'What are the different types of documents available in the '\n",
      "         \"repository (e.g., installation guide, API documentation, developer's \"\n",
      "         'guide)?\\n'\n",
      "         '\\n'\n",
      "         '### Answer:\\n'\n",
      "         'Lamini has documentation on Getting Started, Authentication, '\n",
      "         'Question Answer Model, Python Library, Batching, Error Handling, '\n",
      "         'Advanced topics, and class documentation on LLM Engine available at '\n",
      "         'https://lamini-ai.github.io/.'}\n",
      "{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
      "           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
      "           'Advanced topics, and class documentation on LLM Engine available '\n",
      "           'at https://lamini-ai.github.io/.',\n",
      " 'question': '### Question:\\n'\n",
      "             'What are the different types of documents available in the '\n",
      "             'repository (e.g., installation guide, API documentation, '\n",
      "             \"developer's guide)?\\n\"\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "pprint(finetuning_dataset_text_only[0])\n",
    "pprint(finetuning_dataset_question_answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving new dataset into a new file - jsonlines is the most common (each line )\n",
    "with jsonlines.open(f'lamini_docs_processed.jsonl', \"w\") as writer:\n",
    "    writer.write_all(finetuning_dataset_question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Saving lamini dataset from hugging face - you can also upload your dataset to huggingface and then load it using load_dataset\n",
    "finetuning_dataset_name = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = load_dataset(finetuning_dataset_name)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction fine-tuning: a type of fine tuning that makes an LLM more like a chatbot, and allows it to have conversations. There are also other types of fine-tuning: agentic, copilot (for helping write code), reasoning, routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a fine tuning dataset from Alpaca\n",
    "instruction_tuned_dataset = load_dataset(\"tatsu-lab/alpaca\", split =\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction-tuned dataset:\n",
      "{'instruction': 'Give three tips for staying healthy.', 'input': '', 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}\n",
      "{'instruction': 'What are the three primary colors?', 'input': '', 'output': 'The three primary colors are red, blue, and yellow.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.'}\n",
      "{'instruction': 'Describe the structure of an atom.', 'input': '', 'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Response:\\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}\n",
      "{'instruction': 'How can we reduce air pollution?', 'input': '', 'output': 'There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Response:\\nThere are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.'}\n",
      "{'instruction': 'Describe a time when you had to make a difficult decision.', 'input': '', 'output': 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe a time when you had to make a difficult decision.\\n\\n### Response:\\nI had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'}\n"
     ]
    }
   ],
   "source": [
    "# Taking a look at a few examples from the dataset\n",
    "m = 5\n",
    "print(\"Instruction-tuned dataset:\")\n",
    "top_m = list(itertools.islice(instruction_tuned_dataset, m))\n",
    "for j in top_m:\n",
    "  print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Give three tips for staying healthy.',\n",
       "  'input': '',\n",
       "  'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       "  'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'},\n",
       " {'instruction': 'What are the three primary colors?',\n",
       "  'input': '',\n",
       "  'output': 'The three primary colors are red, blue, and yellow.',\n",
       "  'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat are the three primary colors?\\n\\n### Response:\\nThe three primary colors are red, blue, and yellow.'},\n",
       " {'instruction': 'Describe the structure of an atom.',\n",
       "  'input': '',\n",
       "  'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.',\n",
       "  'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe the structure of an atom.\\n\\n### Response:\\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'},\n",
       " {'instruction': 'How can we reduce air pollution?',\n",
       "  'input': '',\n",
       "  'output': 'There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.',\n",
       "  'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHow can we reduce air pollution?\\n\\n### Response:\\nThere are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances.'},\n",
       " {'instruction': 'Describe a time when you had to make a difficult decision.',\n",
       "  'input': '',\n",
       "  'output': 'I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.',\n",
       "  'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe a time when you had to make a difficult decision.\\n\\n### Response:\\nI had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top_m is a list of dictionaries, with all the instructions, input, output, and text in the fine tuning dataset \n",
    "top_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates - this was used in the Alpaca dataset for the purpose of fine-tuning\n",
    "prompt_template_with_input = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding data to the prompts \n",
    "processed_data = []\n",
    "for j in top_m:\n",
    "    if not j[\"input\"]: #if input is empty\n",
    "        processed_prompt = prompt_template_without_input.format(instruction=j[\"instruction\"])\n",
    "    else:\n",
    "        processed_prompt = prompt_template_with_input.format(instructions=j[\"instruction\"], input=j[\"input\"])\n",
    "\n",
    "    processed_data.append({\"input\": processed_prompt, \"output\": j[\"output\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Below is an instruction that describes a task. Write a response '\n",
      "          'that appropriately completes the request.\\n'\n",
      "          '\\n'\n",
      "          '### Instruction:\\n'\n",
      "          'Give three tips for staying healthy.\\n'\n",
      "          '\\n'\n",
      "          '### Response:',\n",
      " 'output': '1.Eat a balanced diet and make sure to include plenty of fruits '\n",
      "           'and vegetables. \\n'\n",
      "           '2. Exercise regularly to keep your body active and strong. \\n'\n",
      "           '3. Get enough sleep and maintain a consistent sleep schedule.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(processed_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data \n",
    "with jsonlines.open(f'alpaca_processed.jsonl', \"w\") as writer:\n",
    "    writer.write_all(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 52002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Note that this current dataset, with the prompt template, has been loaded onto HuggingFace already. We can access with:\n",
    "\n",
    "dataset_path_hf = \"lamini/alpaca\"\n",
    "dataset_hf = load_dataset(dataset_path_hf)\n",
    "print(dataset_hf)\n",
    "\n",
    "# We could also upload our own to hugging face \n",
    "\n",
    "# !pip install huggingface_hub\n",
    "# !huggingface-cli login\n",
    "\n",
    "# import pandas as pd\n",
    "# import datasets\n",
    "# from datasets import Dataset\n",
    "\n",
    "# finetuning_dataset = Dataset.from_pandas(pd.DataFrame(data=finetuning_dataset))\n",
    "# finetuning_dataset.push_to_hub(dataset_path_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try some smaller models - will use a 70 million param model in this case, but something like llama-7 has 7 billion params which is much larger, and something like gpt-3 has closer to 70 billion params\\\n",
    "\\\n",
    "The from_pretrained() method is used to load pre-trained models or tokenizers so you can use base LLMs and further fine tune for your own use cases.\\\n",
    "\\\n",
    "The model pythia-70m has not been instruction fine-tuned, and will be the model we will be finetuning ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\") \n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    # Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors = \"pt\",\n",
    "        truncation = True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    # Generate \n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids = input_ids.to(device),\n",
    "        max_length=max_output_tokens\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "    # Strip the prompt \n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "    return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Can Lamini generate technical documentation or user manuals for software projects?', 'answer': 'Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.', 'input_ids': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [5804, 418, 4988, 74, 6635, 7681, 10097, 390, 2608, 11595, 84, 323, 3694, 6493, 32, 4374, 13, 418, 4988, 74, 476, 6635, 7681, 10097, 285, 2608, 11595, 84, 323, 3694, 6493, 15, 733, 4648, 3626, 3448, 5978, 5609, 281, 2794, 2590, 285, 44003, 10097, 326, 310, 3477, 281, 2096, 323, 1097, 7681, 285, 1327, 14, 48746, 4212, 15, 831, 476, 5321, 12259, 247, 1534, 2408, 273, 673, 285, 3434, 275, 6153, 10097, 13, 6941, 731, 281, 2770, 327, 643, 7794, 273, 616, 6493, 15]}\n"
     ]
    }
   ],
   "source": [
    "test_sample = finetuning_dataset[\"test\"][0]\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can Lamini generate technical documentation or user manuals for software projects?\n"
     ]
    }
   ],
   "source": [
    "print(test_sample[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nI have a question about the following:\\n\\nHow do I get the correct documentation to work?\\n\\nA:\\n\\nI think you need to use the following code:\\n\\nA:\\n\\nYou can use the following code to get the correct documentation.\\n\\nA:\\n\\nYou can use the following code to get the correct documentation.\\n\\nA:\\n\\nYou can use the following'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try this pythia-70m model that has not yet been finetuned and see what kind of output it gives us\n",
    "inference(test_sample[\"question\"], model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the model output is not very good. It is in english, and seems to have picked up the word \"documentation\", but that is about the extent of its understanding. Lets take a look at what a fine tuned model using the same pythia-70m LLM as a base model looks like. \\\n",
    "\\\n",
    "You can find the model on Hugging Face \\\n",
    "\\\n",
    "lamini/lamini_docs_finetuned is a model that is on HuggingFace (https://huggingface.co/lamini/lamini_docs_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, Lamini can generate technical documentation or user manuals for software projects. This can be achieved by providing a prompt for a specific technical question or question to the LLM Engine, or by providing a prompt for a specific technical question or question. Additionally, Lamini can be trained on specific technical questions or questions to help users understand the process and provide feedback to the LLM Engine. Additionally, Lamini'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "inference(test_sample[\"question\"], model=instruction_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fine tune this pythia-70m model ourselves, and train it so that it can learn the context of the lamini documentation and answer questions.\\\n",
    "\\\n",
    "\\\n",
    "There are steps for preparing data for finetuning a LLM:\n",
    "1. Collect instruction-response pairs\n",
    "2. Concatenate pairs, and add prompt templates if required\n",
    "3. Tokenize - pad and truncate\n",
    "4. Split data into test and train\n",
    "\\\n",
    "\\\n",
    "Lets first take a look at our tokenizer that we have defined. We will look at some toy examples first, before we go on and work on the dataset. We used tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12764, 13, 849, 403, 368, 32], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi, how are you?\"\n",
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each LLM is associated with a tokenizer --> giving the wrong tokenizer will confuse the model, so you will have to make sure you use the correct tokenizer. AutoTokenizer class from transformers will automatically help you tokenize your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12764, 13, 849, 403, 368, 32]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, how are you?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1], [1]]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing multiple texts\n",
    "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
    "tokenizer(list_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything in a batch should be the same length - so they should have the same length for input_ids. We will use padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts = tokenizer(list_texts)\n",
    "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using padding:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175, 0, 0, 0], [4374, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Padding - ensure input sequences are of the same length \n",
    "# eos_token marks the end of sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_texts_longest = tokenizer(list_texts, padding = True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_length is the maximum length it can handle and take in. There is a limit to the length. Truncation makes the longer, encoded texts shorter, so that it can fit into the model. In the below code, the max length has been set to 3, and the truncation has been set to true. This means that only the first 3 input_ids will shown if there are more than 3 input_ids. The default is right side truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12764, 13, 849], [42, 1353, 1175], [4374]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncation\n",
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "encoded_texts_truncation[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[403, 368, 32], [42, 1353, 1175], [4374]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using left side truncation\n",
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "encoded_texts_truncation_left[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[403, 368, 32], [42, 1353, 1175], [4374, 0, 0]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding and truncation\n",
    "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "encoded_texts_both[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets move on to our dataset, and prepare our instruction dataset for finetuning. Preparing our dataset - this will be the instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset files\n",
    "filename = \"lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)\n",
    "examples = instruction_dataset_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the instruction dataset, there are some that are question-answer, instruction-response, and input-output\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"hydrating the prompts\" - putting the data into the prompt templates\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "    question = examples[\"question\"][i]\n",
    "    answer = examples[\"answer\"][i]\n",
    "    text_with_prompt_template = prompt_template.format(question=question)\n",
    "    finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': \"### Question:\\nWhat are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\\n\\n### Answer:\",\n",
       "  'answer': 'Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.'},\n",
       " {'question': '### Question:\\nWhat is the recommended way to set up and configure the code repository?\\n\\n### Answer:',\n",
       "  'answer': 'Lamini can be downloaded as a python package and used in any codebase that uses python. Additionally, we provide a language agnostic REST API. We’ve seen users develop and train models in a notebook environment, and then switch over to a REST API to integrate with their production environment.'},\n",
       " {'question': '### Question:\\nHow can I find the specific documentation I need for a particular feature or function?\\n\\n### Answer:',\n",
       "  'answer': 'You can ask this model about documentation, which is trained on our publicly available docs and source code, or you can go to https://lamini-ai.github.io/.'},\n",
       " {'question': \"### Question:\\nDoes the documentation include explanations of the code's purpose and how it fits into a larger system?\\n\\n### Answer:\",\n",
       "  'answer': 'Our documentation provides both real-world and toy examples of how one might use Lamini in a larger system. In particular, we have a walkthrough of how to build a Question Answer model available here: https://lamini-ai.github.io/example/'},\n",
       " {'question': '### Question:\\nDoes the documentation provide information about any external dependencies or libraries used by the code?\\n\\n### Answer:',\n",
       "  'answer': 'External dependencies and libraries are all available on the Python package hosting website Pypi at https://pypi.org/project/lamini/'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the first 5 data points in the dataset of question and answer\n",
    "finetuning_dataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4118, 19782,    27,   187,  1276,   403,   253,  1027,  3510,\n",
       "          273,  7177,  2130,   275,   253, 18491,   313,    70,    15,\n",
       "           72,   904, 12692,  7102,    13,  8990, 10097,    13, 13722,\n",
       "          434,  7102,  6177,   187,   187,  4118, 37741,    27,    45,\n",
       "         4988,    74,   556, 10097,   327, 27669, 11075,   264,    13,\n",
       "         5271, 23058,    13, 19782, 37741, 10031,    13, 13814, 11397,\n",
       "           13,   378, 16464,    13, 11759, 10535,  1981,    13, 21798,\n",
       "        12989,    13,   285,   966, 10097,   327, 21708,    46, 10797,\n",
       "         2130,   387,  5987,  1358,    77,  4988,    74,    14,  2284,\n",
       "           15,  7280,    15,   900, 14206]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing a single example - here we return the tensors as a numpy array for simplicity\n",
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "tokenized_inputs = tokenizer(text, return_tensors = \"np\", padding=True)\n",
    "tokenized_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4118, 19782,    27,   187,  1276,   403,   253,  1027,  3510,\n",
       "          273,  7177,  2130,   275,   253, 18491,   313,    70,    15,\n",
       "           72,   904, 12692,  7102,    13,  8990, 10097,    13, 13722,\n",
       "          434,  7102,  6177,   187,   187,  4118, 37741,    27,    45,\n",
       "         4988,    74,   556, 10097,   327, 27669, 11075,   264,    13,\n",
       "         5271, 23058,    13, 19782, 37741, 10031,    13, 13814, 11397,\n",
       "           13,   378, 16464,    13, 11759, 10535,  1981,    13, 21798,\n",
       "        12989,    13,   285,   966, 10097,   327, 21708,    46, 10797,\n",
       "         2130,   387,  5987,  1358,    77,  4988,    74,    14,  2284,\n",
       "           15,  7280,    15,   900, 14206]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max = 2048\n",
    "max_length = min(tokenized_inputs[\"input_ids\"].shape[1], max)\n",
    "tokenized_inputs = tokenizer(text, return_tensors=\"np\", truncation=True, max_length=max_length)\n",
    "tokenized_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make the above process into a function so we can run it on the entire dataset\n",
    "def tokenize_function(examples):\n",
    "   if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "   elif \"instruction\" in examples and \"response\" in examples:\n",
    "      text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "   elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "   else:\n",
    "      text = examples[\"text\"][0]\n",
    "        \n",
    "   tokenizer.pad_token = tokenizer.eos_token\n",
    "   tokenized_inputs = tokenizer(text, return_tensors=\"np\", padding=True)\n",
    "   max = 2048\n",
    "   max_length = min(tokenized_inputs[\"input_ids\"].shape[1], max)\n",
    "   tokenizer.truncation_side = \"left\"\n",
    "   tokenized_inputs = tokenizer(text, return_tensors=\"np\", truncation=True, max_length=max_length)\n",
    "\n",
    "   return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched = True,\n",
    "    batch_size = 1,\n",
    "    drop_last_batch = True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add the \"labels\" column to split the dataset into test and train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Notice that all the datasets have question, answer, input_ids, attention_mask, and labels columns\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps were just manually done on a dataset, so that we can go through the process of what it would be like preparing a dataset for training LLM models. We used a dataset that was already available on huggingface. The exact same dataset can be retrieved from hugging face directly below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Trying out some datasets\n",
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other datasets we can also take a look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "taylor_swift_dataset = \"lamini/taylor_swift\"\n",
    "bts_dataset = \"lamini/bts\"\n",
    "open_llms_dataset = \"lamini/open_llms\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"What is the controversy surrounding Taylor Swift's music and how has it impacted her career?\", 'answer': 'Taylor Swift has been involved in several controversies throughout her career, including her feud with Kanye West and Kim Kardashian, her lawsuit against a radio DJ who allegedly groped her, and her recent feud with Scooter Braun. These controversies have impacted her career in several ways. First, they have made her a more polarizing figure in the music industry, with some fans supporting her and others criticizing her. Second, they have led to a decrease in her popularity among some listeners, particularly those who do not agree with her political views or her actions in the feuds. Finally, they have led to a decrease of her music being played on some radio stations, which has impacted her ability to reach new audiences', 'input_ids': [1276, 310, 253, 16305, 8704, 11276, 24619, 434, 3440, 285, 849, 556, 352, 27857, 617, 5249, 32, 37979, 24619, 556, 644, 3206, 275, 2067, 9474, 447, 4768, 617, 5249, 13, 1690, 617, 40157, 342, 611, 1279, 70, 4255, 285, 10766, 611, 472, 1225, 757, 13, 617, 15091, 1411, 247, 5553, 22533, 665, 14163, 305, 1658, 264, 617, 13, 285, 617, 3332, 40157, 342, 1810, 37449, 47907, 15, 2053, 9474, 447, 452, 27857, 617, 5249, 275, 2067, 4088, 15, 3973, 13, 597, 452, 1160, 617, 247, 625, 6994, 3006, 4677, 275, 253, 3440, 4491, 13, 342, 690, 7458, 8109, 617, 285, 2571, 7291, 3006, 617, 15, 6347, 13, 597, 452, 3977, 281, 247, 6379, 275, 617, 18395, 2190, 690, 30418, 13, 3782, 1110, 665, 513, 417, 5194, 342, 617, 3569, 6849, 390, 617, 5231, 275, 253, 704, 36626, 15, 6610, 13, 597, 452, 3977, 281, 247, 6379, 273, 617, 3440, 1146, 4546, 327, 690, 5553, 10988, 13, 534, 556, 27857, 617, 3745, 281, 3986, 747, 23886], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1276, 310, 253, 16305, 8704, 11276, 24619, 434, 3440, 285, 849, 556, 352, 27857, 617, 5249, 32, 37979, 24619, 556, 644, 3206, 275, 2067, 9474, 447, 4768, 617, 5249, 13, 1690, 617, 40157, 342, 611, 1279, 70, 4255, 285, 10766, 611, 472, 1225, 757, 13, 617, 15091, 1411, 247, 5553, 22533, 665, 14163, 305, 1658, 264, 617, 13, 285, 617, 3332, 40157, 342, 1810, 37449, 47907, 15, 2053, 9474, 447, 452, 27857, 617, 5249, 275, 2067, 4088, 15, 3973, 13, 597, 452, 1160, 617, 247, 625, 6994, 3006, 4677, 275, 253, 3440, 4491, 13, 342, 690, 7458, 8109, 617, 285, 2571, 7291, 3006, 617, 15, 6347, 13, 597, 452, 3977, 281, 247, 6379, 275, 617, 18395, 2190, 690, 30418, 13, 3782, 1110, 665, 513, 417, 5194, 342, 617, 3569, 6849, 390, 617, 5231, 275, 253, 704, 36626, 15, 6610, 13, 597, 452, 3977, 281, 247, 6379, 273, 617, 3440, 1146, 4546, 327, 690, 5553, 10988, 13, 534, 556, 27857, 617, 3745, 281, 3986, 747, 23886]}\n"
     ]
    }
   ],
   "source": [
    "dataset_taylor_swift = datasets.load_dataset(taylor_swift_dataset)\n",
    "print(dataset_taylor_swift[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 783\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 87\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_taylor_swift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'AlekseyKorshuk-chatml-pyg-v1: AlekseyKorshuk-chatml-pyg-v1: AlekseyKorshuk-chatml-pyg-v1: What dataset was used for training?', 'answer': 'The None dataset was used for training.', 'input_ids': [44163, 76, 5462, 44, 641, 73, 2788, 14, 23481, 1686, 14, 4789, 72, 14, 87, 18, 27, 16660, 76, 5462, 44, 641, 73, 2788, 14, 23481, 1686, 14, 4789, 72, 14, 87, 18, 27, 16660, 76, 5462, 44, 641, 73, 2788, 14, 23481, 1686, 14, 4789, 72, 14, 87, 18, 27, 1737, 10895, 369, 908, 323, 3733, 32, 510, 8256, 10895, 369, 908, 323, 3733, 15], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [44163, 76, 5462, 44, 641, 73, 2788, 14, 23481, 1686, 14, 4789, 72, 14, 87, 18, 27, 16660, 76, 5462, 44, 641, 73, 2788, 14, 23481, 1686, 14, 4789, 72, 14, 87, 18, 27, 16660, 76, 5462, 44, 641, 73, 2788, 14, 23481, 1686, 14, 4789, 72, 14, 87, 18, 27, 1737, 10895, 369, 908, 323, 3733, 32, 510, 8256, 10895, 369, 908, 323, 3733, 15]}\n"
     ]
    }
   ],
   "source": [
    "dataset_open_llms = datasets.load_dataset(open_llms_dataset)\n",
    "print(dataset_open_llms[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1001\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 112\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_open_llms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training \n",
    "This next section focuses on model training. We want to take a look at how the code is running under the hood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Lamini' object has no attribute 'load_data_from_jsonlines'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Lamini(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEleutherAI/pythia-70m\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_from_jsonlines\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlamini_docs.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(is_public\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Lamini' object has no attribute 'load_data_from_jsonlines'"
     ]
    }
   ],
   "source": [
    "# model = Lamini(\"EleutherAI/pythia-70m\") \n",
    "# model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "# model.train(is_public=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
